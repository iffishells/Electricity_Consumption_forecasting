{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # Library for data manipulation and analysis\n",
    "import numpy as np  # Library for numerical computing\n",
    "import glob  # Library for file handling\n",
    "import re  # Library for regular expressions\n",
    "from tqdm import tqdm  # Library for creating progress bars\n",
    "\n",
    "import plotly.graph_objects as go  # Library for creating interactive plots\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn import preprocessing  # Library for data preprocessing\n",
    "import keras  # Deep learning library\n",
    "from keras.models import Sequential  # Sequential model for stacking layers\n",
    "from keras.layers.core import Dense, Dropout, Activation  # Layers for fully connected neural networks\n",
    "from keras.layers import LSTM  # LSTM layer for sequence modeling\n",
    "from keras.models import load_model  # Loading pre-trained models\n",
    "import matplotlib.pyplot as plt  # Library for basic data visualization\n",
    "import h5py  # Library for handling large datasets\n",
    "import datetime  # Library for date and time operations\n",
    "import tensorflow as tf  # Deep learning library\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For preprocessing and modeling**\n",
    "* sklearn.preprocessing is used for data preprocessing tasks, such as scaling or encoding categorical variables.\n",
    "* keras is a deep learning library that provides tools for building and training neural networks.\n",
    "* Sequential is a type of model in Keras that allows stacking layers sequentially.\n",
    "* Dense represents a fully connected layer in a neural network.\n",
    "* Dropout is used for regularization by randomly setting input units to 0 during training to prevent overfitting.\n",
    "* Activation defines the activation function to introduce non-linearity in the neural network.\n",
    "* LSTM is a type of recurrent neural network layer used for sequence modeling.\n",
    "* load_model is used to load pre-trained models.\n",
    "* matplotlib.pyplot is imported as plt for basic data visualization.\n",
    "* h5py is used for handling large datasets stored in the Hierarchical Data Format (HDF5).\n",
    "* datetime is used for handling date and time information.\n",
    "* tensorflow is imported as tf, which is a deep learning library used for various operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  # Import the TensorFlow library\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))  # Print the number of available GPUs\n",
    "\n",
    "# tf.config.list_physical_devices('GPU') retrieves the list of physical devices (GPUs) available for TensorFlow to use.\n",
    "# len(tf.config.list_physical_devices('GPU')) calculates the length of the list of GPUs.\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU'))) prints the number of available GPUs.\n",
    "\n",
    "# Summary :\n",
    "# This code is useful when you want to check the number of GPUs available for your TensorFlow environment. \n",
    "# GPUs are commonly used in deep learning for faster computations on large neural networks. \n",
    "# Knowing the number of available GPUs can help determine the hardware resources and parallel\n",
    "# processing capabilities of your system when running TensorFlow-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code defines a function named plot that takes four parameters: df, x_feature_name,\n",
    "# y_feature_name, and title. The function is designed to plot data from a dataframe\n",
    "def plot(df,x_feature_name,y_feature_name,title):\n",
    "    \"\"\"\n",
    "    This function takes two dataframes as input and plots the number of calls per day and per week.\n",
    "\n",
    "    Args:\n",
    "    daily_df (pandas.DataFrame): A dataframe containing daily call data.\n",
    "    weekly_df (pandas.DataFrame): A dataframe containing weekly call data.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # A new instance of the go.Figure() class from the plotly.graph_objects library is created. This will be used to create the plot\n",
    "    fig = go.Figure()\n",
    "    # Add a trace for daily calls\n",
    "    # A trace is added to the figure using the go.Scatter() class from plotly.graph_objects. \n",
    "    # It specifies the x and y data for the plot, assigns a name to the trace, \n",
    "    # and sets the mode to display lines and markers.\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[x_feature_name],\n",
    "            y=df[y_feature_name],\n",
    "            name=y_feature_name,\n",
    "            mode='lines+markers'\n",
    "        ))\n",
    "\n",
    " \n",
    "\n",
    "    # Update xaxis properties\n",
    "    # The x-axis and y-axis titles are updated using the update_xaxes() and update_yaxes() methods of the figure object.\n",
    "    fig.update_xaxes(title_text='Date')\n",
    "\n",
    "    # Update yaxis properties\n",
    "    fig.update_yaxes(title_text=y_feature_name)\n",
    "\n",
    "    # Update title and height\n",
    "    # The layout of the figure is updated using the update_layout() method. The title, height, and width of the plot are set.\n",
    "    fig.update_layout(\n",
    "        title=f'{title}',\n",
    "        height=500,\n",
    "        width=1200\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    # The plot is displayed using the show() method of the figure object.\n",
    "    fig.show()\n",
    "\n",
    "    # Write the plot to an HTML file\n",
    "    # fig.write_html(f'Visualization/btc.html')\n",
    "# Summary:\n",
    "# The code defines a function plot() that takes in a dataframe (df), x-axis feature name (x_feature_name),\n",
    "# y-axis feature name (y_feature_name), and a title. Inside the function, a plot is created using plotly.graph_objects. \n",
    "# The provided x and y data from the dataframe are added as a trace to the plot. \n",
    "# The x-axis and y-axis titles are updated, along with the plot title, height, and width. \n",
    "# The plot is then displayed using fig.show(). There is an optional commented-out line that suggests writing the plot to an HTML file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code defines a function named downsample_dataframe that takes in three parameters: df (the DataFrame to be downsampled), \n",
    "# downsampling_frequency (the desired frequency to downsample the DataFrame to), \n",
    "# and fill_method (the method to use for filling missing values).\n",
    "# The function aims to downsample the DataFrame and fill any missing value\n",
    "def downsample_dataframe(df, downsampling_frequency, fill_method='mean'):\n",
    "  \"\"\"\n",
    "  Downsamples a DataFrame and fills missing values.\n",
    "\n",
    "  Args:\n",
    "    df: The DataFrame to downsample.\n",
    "    downsampling_frequency: The frequency to downsample the DataFrame to.\n",
    "    fill_method: The method to use to fill missing values.\n",
    "\n",
    "  Returns:\n",
    "    The downsampled DataFrame.\n",
    "  \"\"\"\n",
    "\n",
    "  # Convert the Timestamp column to a datetime object.\n",
    "  # This line converts the 'Timestamp' column in the DataFrame to a datetime object using the pd.to_datetime() function from the pandas library.\n",
    "  df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "  # Set the index of the DataFrame to the Timestamp column.\n",
    "  # This line sets the 'Timestamp' column as the index of the DataFrame using the set_index() method\n",
    "  df = df.set_index('Timestamp')\n",
    "\n",
    "  # Fill the missing values.\n",
    "  # This block of code fills the missing values in the DataFrame based on the specified fill_method. \n",
    "  # If fill_method is set to 'mean', the missing values are filled with the mean value of each \n",
    "  # column using the fillna() method and df.mean(). If fill_method is set to 'median', \n",
    "  # he missing values are filled with the median value of each column using df.median(). \n",
    "  # If an invalid fill_method is provided, a ValueError is raised\n",
    "  if fill_method == 'mean':\n",
    "    df = df.fillna(df.mean())\n",
    "  elif fill_method == 'median':\n",
    "    df = df.fillna(df.median())\n",
    "  else:\n",
    "    raise ValueError('Invalid fill_method: {}'.format(fill_method))\n",
    "\n",
    "  # Downsample the DataFrame.\n",
    "  # This line downsamples the DataFrame to the specified downsampling_frequency using the resample() method with \n",
    "  # downsampling_frequency as the argument. The mean() method is then applied to calculate the mean \n",
    "  # value for each downsampled interval. The resulting downsampled DataFrame is returned as the output of the function.\n",
    "  df = df.resample(downsampling_frequency).mean()\n",
    "  return df\n",
    "\n",
    "# Summary:\n",
    "# The code defines a function downsample_dataframe() that takes in a DataFrame, downsampling frequency, \n",
    "# and fill method. The function converts the 'Timestamp' column to a datetime object, sets it as the index, \n",
    "# fills missing values based on the specified fill method (mean or median), and then downsamples the DataFrame \n",
    "# to the desired frequency using the mean value within each downsampled interval. \n",
    "# The resulting downsampled DataFrame is returned as the output of the function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines use the glob module to find all file paths that match the pattern 'GreenD_reduced_version_03/*.csv'. \n",
    "# The resulting file paths are stored in path_lists.\n",
    "# Then, the file paths are sorted alphabetically and assigned to sorted_file_paths\n",
    "path_lists = glob.glob('GreenD_reduced_version_03/'+'*.csv')\n",
    "sorted_file_paths = sorted(path_lists)\n",
    "\n",
    "\n",
    "# This code defines a function extract_digits that takes a string as input and \n",
    "# uses regular expressions (re.findall()) to extract digits from the string. \n",
    "# It returns the extracted digits as an integer if they exist, otherwise it returns 0.\n",
    "# Then, the sorted() function is used with key=extract_digits to sort the file\n",
    "# paths based on the digits extracted from each path. Finally, the sorted file paths are printed\n",
    "def extract_digits(string):\n",
    "    # Extract digits from the string using regular expression\n",
    "    digits = re.findall(r'\\d+', string)\n",
    "    return int(digits[0]) if digits else 0\n",
    "\n",
    "sorted_file_paths = sorted(path_lists, key=extract_digits)\n",
    "print(sorted_file_paths)\n",
    "\n",
    "\n",
    "# In this block of code, an empty list conct_list is created. Then, a loop is executed over each sorted\n",
    "# file path using sorted_file_paths. Inside the loop, each CSV file is read using pd.read_csv(),\n",
    "# and the 'Timestamp' column is converted to a datetime object using pd.to_datetime() w\n",
    "# ith the utc=True parameter and unit='s' to indicate that the values are in Unix timestamp format. \n",
    "# The 'Timestamp' column is then formatted to a specific string format '%Y-%m-%d %H:%M:%S' \n",
    "# using the dt.strftime() function. The resulting dataframe is appended to the conct_list\n",
    "\n",
    "conct_list = []\n",
    "for path in tqdm(sorted_file_paths ,desc='processing'):\n",
    "    \n",
    "    data = pd.read_csv(path)\n",
    "    data['Timestamp'] = pd.to_datetime(data['Timestamp'],utc=True,unit='s')#format='%Y-%m-%d %H-%M-%S')\n",
    "    data['Timestamp'] = data['Timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    conct_list.append(data)\n",
    "    \n",
    "# The !mkdir command creates a directory named 'Combined-Dataset-version-03' \n",
    "# if it doesn't already exist. Then, pd.concat() is used to concatenate all \n",
    "# the dataframes in conct_list along the rows, creating a single combined dataframe named df\n",
    "!mkdir 'Combined-Dataset-version-03'\n",
    "df = pd.concat(conct_list)\n",
    "\n",
    "\n",
    "# Summary:\n",
    "# The code reads multiple CSV files located in the 'GreenD_reduced_version_03/' directory. \n",
    "# The file paths are sorted in ascending order based on the digits extracted from each path using a custom function.\n",
    "# Each file is then loaded into a dataframe, and the 'Timestamp' column is converted to a datetime object and formatted.\n",
    "# The dataframes are appended to a list. A new directory named 'Combined-Dataset-version-03' is created,\n",
    "# and all the dataframes in the list are concatenated into a single dataframe named df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='Timestamp',inplace=True)\n",
    "# This line of code sorts the DataFrame df based on the values in the 'Timestamp' column in ascending order. \n",
    "# The sort_values() function is used to perform the sorting operation. The by parameter specifies \n",
    "# the column to sort by, which in this case is 'Timestamp'. The inplace=True parameter is\n",
    "# used to modify the DataFrame df in place, meaning the changes are applied directly to the DataFrame instead of creating a new sorted DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Customize the histogram plot\n",
    "# This line creates a new figure object with a specific size of 8 inches by 6 inches. The figsize parameter is used to set the width and height of the figure in inches.\n",
    "plt.figure(figsize=(8, 6))  # Set the figure size\n",
    "\n",
    "# Plot the histogram\n",
    "# This line plots a histogram using the plot.hist() method of the DataFrame df. It specifies the following parameters:\n",
    "\n",
    "# bins=10: Sets the number of bins (bars) in the histogram to 10.\n",
    "# edgecolor='black': Sets the color of the edges of the bars to black.\n",
    "# alpha=0.7: Sets the transparency of the bars to 0.7 (partially transparent).\n",
    "# color='steelblue': Sets the color of the bars to steel blue.\n",
    "# linewidth=2: Sets the width of the bar edge lines to 2 pixels.\n",
    "df.plot.hist(\n",
    "    bins=10,                   # Number of bins\n",
    "    edgecolor='black',         # Color of the edges of the bars\n",
    "    alpha=0.7,                 # Transparency of the bars\n",
    "    color='steelblue',         # Color of the bars\n",
    "    linewidth=2,               # Width of the bar edge lines\n",
    ")\n",
    "\n",
    "# Customize the plot labels and title\n",
    "plt.title('Histogram of Five Second Sampling Frequency Data')    # Set the title\n",
    "plt.xlabel('Values')              # Set the x-axis label\n",
    "plt.ylabel('Frequency')           # Set the y-axis label\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig('5sec.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Summary:\n",
    "# The code creates a histogram plot using the plot.hist() method of the DataFrame df. \n",
    "# It customizes the plot by specifying the number of bins, edge color, transparency, \n",
    "# bar color, and line width. The plot is given a title, x-axis label, and y-axis label. \n",
    "# The resulting plot is saved as an image file named '5sec.png', and then it is displayed on the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Downsample the DataFrame to 1-hour frequency and fill missing values using mean imputation.\n",
    "# df_1_hour = downsample_dataframe(df, '1H', fill_method='mean'): The downsample_dataframe() function is called to downsample the DataFrame df to a 1-hour frequency and fill missing values using mean imputation. The resulting downsampled DataFrame is assigned to the variable df_1_hour.\n",
    "# df_1_hour.reset_index('Timestamp', inplace=True): The index of the DataFrame df_1_hour is reset, converting the 'Timestamp' column from the index back to a regular column. The reset_index() method is used, and the inplace=True parameter is set to modify the DataFrame in place.\n",
    "# df_1_hour.fillna(method='ffill', inplace=True): Missing values in the DataFrame df_1_hour are filled using forward fill (ffill) method, which propagates the last observed value forward. The fillna() method is used, and the inplace=True parameter is set to modify the DataFrame in place\n",
    "\n",
    "df_1_hour = downsample_dataframe(df, '1H', fill_method='mean')\n",
    "df_1_hour.reset_index('Timestamp',inplace=True)\n",
    "df_1_hour.fillna(method='ffill',inplace=True)\n",
    "\n",
    "\n",
    "# df_daily = downsample_dataframe(df, '24H', fill_method='mean'): The downsample_dataframe() function is called to downsample the DataFrame df to a daily frequency (24 hours) and fill missing values using mean imputation. The resulting downsampled DataFrame is assigned to the variable df_daily.\n",
    "# df_daily.reset_index('Timestamp', inplace=True): The index of the DataFrame df_daily is reset, converting the 'Timestamp' column from the index back to a regular column. The reset_index() method is used, and the inplace=True parameter is set to modify the DataFrame in place.\n",
    "# df_daily.fillna(method='ffill', inplace=True): Missing values in the DataFrame df_daily are filled using forward fill (ffill) method, which propagates the last observed value forward. The fillna() method is used, and the inplace=True parameter is set to modify the DataFrame in place.\n",
    "# Downsample the DataFrame to 1-hour frequency and fill missing values using mean imputation.\n",
    "df_daily = downsample_dataframe(df, '24H', fill_method='mean')\n",
    "df_daily.reset_index('Timestamp',inplace=True)\n",
    "df_daily.fillna(method='ffill',inplace=True)\n",
    "\n",
    "# Summary:\n",
    "# The code performs downsampling of the original DataFrame df to different \n",
    "# frequencies (1-hour and daily) using the downsample_dataframe() function.\n",
    "# Missing values are filled using mean imputation, and the resulting downsampled\n",
    "# DataFrames (df_1_hour and df_daily) are modified in place to reset the index and fill missing values using forward fill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Customize the histogram plot\n",
    "plt.figure(figsize=(8, 6))  # Set the figure size\n",
    "\n",
    "# Plot the histogram\n",
    "df_1_hour.plot.hist(\n",
    "    bins=10,                   # Number of bins\n",
    "    edgecolor='black',         # Color of the edges of the bars\n",
    "    alpha=0.7,                 # Transparency of the bars\n",
    "    color='steelblue',         # Color of the bars\n",
    "    linewidth=2,               # Width of the bar edge lines\n",
    ")\n",
    "\n",
    "# Customize the plot labels and title\n",
    "plt.title('Histogram of One Hour Sampling Frequency Data')    # Set the title\n",
    "plt.xlabel('Values')              # Set the x-axis label\n",
    "plt.ylabel('Frequency')           # Set the y-axis label\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig('1-Hours.png')\n",
    "\n",
    "plt.show()\n",
    "# Summary:\n",
    "# The code creates a histogram plot using the plot.hist() method of the DataFrame df_1_hour. \n",
    "# It customizes the plot by specifying the number of bins, edge color, transparency, \n",
    "# bar color, and line width. The plot is given a title, x-axis label, and y-axis label. \n",
    "# The resulting plot is saved as an image file named '1-Hours.png', and then it is displayed on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1_hour.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "# This code defines a function named create_dataset that takes a dataset and an optional parameter look_back as input. It returns two values as a tuple\n",
    "def create_dataset(dataset, look_back=15):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        dataset (_type_): _description_\n",
    "        look_back (int, optional): _description_. Defaults to 15.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # These lines initialize empty lists dataX and dataY\n",
    "    dataX, dataY = [], []\n",
    "    # This line starts a loop that iterates over the range from 0 to len(dataset)-look_back-1.\n",
    "    # It will loop over the indices of the dataset, excluding the last look_back + 1 elements.\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        # These lines extract a subsequence of length look_back from the dataset starting from index i. \n",
    "        # It assigns the subsequence to variable a and appends it to the dataX list.\n",
    "        # It also appends the element at index i + look_back from the dataset to the dataY list\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "        # print('Value of a : {}'.format(a))\n",
    "        # print('Value of y : {}'.format(dataset[i + look_back, 0]))\n",
    "        \n",
    "    # This line converts dataX and dataY lists into NumPy arrays and returns them as the output of the functio\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "# Summary:\n",
    "# The create_dataset function takes a dataset as input and creates input-output pairs for a sequence prediction problem. \n",
    "# It uses a sliding window approach to extract subsequences of length look_back from the dataset and assigns\n",
    "# them as input sequences (dataX). The element following each input sequence is assigned as the corresponding output value (dataY). \n",
    "# The function returns the input and output sequences as NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = df_1_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(data_df,x_feature_name='Timestamp',y_feature_name='Summe',title='Sampling Frequency 1 Hours')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(df_daily,x_feature_name='Timestamp',y_feature_name='Summe',title='Sampling Frequency Daily')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "data_df = df_1_hour # Your data dataframe\n",
    "# These lines perform feature scaling on the 'Summe' column of the DataFrame data_df \n",
    "# using Min-Max scaling. It creates an instance of MinMaxScaler and fits it to the data, \n",
    "# transforming the values to the range [0, 1]. The scaled data is assigned to the variable dataset\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = min_max_scaler.fit_transform(data_df['Summe'].values.reshape(-1, 1))\n",
    "\n",
    "# split into train, validation, and test sets\n",
    "# These lines split the dataset into train, validation, and test sets based on specified proportions.\n",
    "# It calculates the sizes of each set and assigns the corresponding subsets of dataset to train_data, val_data, and test_data\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "val_size = int(len(dataset) * 0.2)\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_data = dataset[:train_size]\n",
    "val_data = dataset[train_size:train_size+val_size]\n",
    "test_data = dataset[train_size+val_size:]\n",
    "\n",
    "# create train, validation, and test datasets\n",
    "# These lines call the create_dataset function to create input-output pairs for the train, validation, \n",
    "# and test sets. The function takes the respective subsets of data (train_data, val_data, test_data) \n",
    "# and a look_back parameter, and returns input (x) and output (y) sequences. \n",
    "# The input-output pairs are assigned to x_train, y_train, x_val, y_val, x_test, and y_test\n",
    "look_back = 20\n",
    "x_train, y_train = create_dataset(train_data, look_back)\n",
    "x_val, y_val = create_dataset(val_data, look_back)\n",
    "x_test, y_test = create_dataset(test_data, look_back)\n",
    "\n",
    "\n",
    "# reshape the input data\n",
    "# These lines reshape the input sequences (x_train, x_val, x_test) to match the required input shape for the LSTM model. \n",
    "# The new shape is (batch_size, timesteps, features), where batch_size is the number of samples,\n",
    "# timesteps is the number of time steps, and features is the number of features.\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], 1, x_val.shape[1]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "\n",
    "# print the sizes of the datasets\n",
    "print('Training Data Size:', train_data.shape)\n",
    "print('Validation Data Size:', val_data.shape)\n",
    "print('Testing Data Size:', test_data.shape)\n",
    "\n",
    "# Summary:\n",
    "# The code performs the following steps:\n",
    "\n",
    "# Assigns a DataFrame to data_df.\n",
    "# Performs Min-Max scaling on a specific column of the DataFrame.\n",
    "# Splits the scaled dataset into train, validation, and test sets.\n",
    "# Calls the create_dataset function to create input-output pairs for the train, validation, and test sets.\n",
    "# Reshapes the input sequences to match the required shape for the LSTM model.\n",
    "# Prints the sizes of the train, validation, and test datasets.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building | Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if GPU is available\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"GPU is available.\")\n",
    "    print(\"List of physical GPUs:\")\n",
    "    for device in physical_devices:\n",
    "        print(device)\n",
    "else:\n",
    "    print(\"GPU is not available. Using CPU instead.\")\n",
    "    \n",
    "    \n",
    "# Summary:\n",
    "# The code checks if a GPU is available and sets the memory growth option if it is.\n",
    "# It then prints the availability and a list of physical GPUs if present\n",
    "# . If no GPU is available, it prints a message indicating the use of the CPU instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Set the model parameters\n",
    "# These lines define the model parameters, including the number of epochs, batch size, number of LSTM layers, and the input window size.\n",
    "num_epochs = 100\n",
    "batch_size = 16\n",
    "num_layers = 4\n",
    "input_window=20\n",
    "\n",
    "# Create the LSTM model\n",
    "# These lines define the architecture of the LSTM model using the Sequential API.\n",
    "# The model consists of multiple LSTM layers with different configurations,\n",
    "# followed by a Dense layer. The input shape of the model is defined based on the look_back value.\n",
    "model = Sequential()\n",
    "model.add(LSTM(input_window, input_shape=(1, look_back), return_sequences=True))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(LSTM(30, return_sequences=True))\n",
    "model.add(LSTM(5, return_sequences=True))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "# This line compiles the model by specifying the loss function and optimizer to use during training.\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "# Get the model's parameters\n",
    "# This line calculates the total number of trainable parameters in the model and assigns it to the model_params variable.\n",
    "model_params = model.count_params()\n",
    "\n",
    "# Create the model name based on parameters and training settings\n",
    "# This line creates a string that represents the model name based on the model's parameters and training settings.\n",
    "model_name = f\"model_{num_epochs}_epochs_{batch_size}_batch_{num_layers}_layers_{model_params}_params_input_window_{input_window}\"\n",
    "\n",
    "\n",
    "# Define early stopping callback\n",
    "# This line defines an EarlyStopping callback that monitors the validation loss and stops training if the loss does not improve for a specified number of epochs.\n",
    "early_stopping = EarlyStopping(patience=10,\n",
    "                               monitor = 'val_loss', \n",
    "                               restore_best_weights=True)\n",
    "\n",
    "# Define TensorBoard callback with model-specific log folder\n",
    "# This line defines a TensorBoard callback that logs training information and metrics to a specified directory.\n",
    "tb_callback = TensorBoard(log_dir=f'logs/{model_name}/', write_graph=True, update_freq='epoch')\n",
    "\n",
    "# Define model checkpoint callback with model-specific filename\n",
    "# This line defines a ModelCheckpoint callback that saves the model's weights to a file after each epoch, only saving the best weights based on the validation loss.\n",
    "checkpoint_callback = ModelCheckpoint(f'checkpoints/{model_name}_{{epoch:02d}}.h5', save_weights_only=True, save_best_only=True)\n",
    "\n",
    "# Train the model with early stopping, checkpoints, and TensorBoard\n",
    "# This line trains the model using the fit function, providing the training and validation data, \n",
    "# number of epochs, batch size, verbosity level, and the defined callbacks for early stopping,\n",
    "# TensorBoard, and model checkpointing. The training history is stored in the history variable\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), \n",
    "                    epochs=num_epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    verbose=2,\n",
    "                    callbacks=[early_stopping, tb_callback, checkpoint_callback])\n",
    "\n",
    "# Print model summary\n",
    "# This line prints a summary of the model's architecture and the number of trainable parameters.\n",
    "print(model.summary())\n",
    "\n",
    "# Summary:\n",
    "# The code builds and trains an LSTM model with specified parameters and settings. It defines the model architecture, \n",
    "# compiles the model, sets up callbacks for early stopping, TensorBoard, and model checkpointing, \n",
    "# and trains the model using the provided data. Finally, it prints a summary of the model's architecture and parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import joblib\n",
    "except:\n",
    "    !pip install joblib\n",
    "    import joblib\n",
    "    # Save the model\n",
    "joblib.dump(model, 'model.pkl')\n",
    "# Load the model\n",
    "loaded_model = joblib.load('model.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 67\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m rescaled_predictions\n\u001b[1;32m     64\u001b[0m \u001b[39m# Example usage\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[39m# Assuming you have a trained model and a scaler object\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m model \u001b[39m=\u001b[39m model\n\u001b[1;32m     68\u001b[0m scaler \u001b[39m=\u001b[39m min_max_scaler\n\u001b[1;32m     70\u001b[0m \u001b[39m# Assuming you have the input data\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_model_and_scaler(model_path=None, scaler_path=None):\n",
    "    # Use default paths if not provided\n",
    "    model_path = model_path# or MODEL_PATH\n",
    "    scaler_path = scaler_path #or SCALER_PATH\n",
    "\n",
    "    # Load the model from the .pkl file\n",
    "    with open(model_path, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "\n",
    "    # Load the scaler from the .pkl file\n",
    "    with open(scaler_path, 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "\n",
    "    return loaded_model, scaler\n",
    "\n",
    "\n",
    "def predict_next_points(model_path, scaler_path, input_data, num_predictions=24):\n",
    "    \"\"\"\n",
    "    Predicts the next points in a sequence using the provided model.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the saved model file (.pkl).\n",
    "        scaler_path (str): Path to the saved scaler file (.pkl).\n",
    "        input_data (ndarray): Input sequence of points.\n",
    "        num_predictions (int): Number of points to predict. Default is 24.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Predicted sequence of points.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the model from the .pkl file\n",
    "    with open(model_path, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "\n",
    "    # Load the scaler from the .pkl file\n",
    "    with open(scaler_path, 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "\n",
    "    # Reshape the input data\n",
    "    input_data = np.array(input_data).reshape(1, 1, -1)\n",
    "\n",
    "    # Predict the next points iteratively\n",
    "    predictions = []\n",
    "    for _ in range(num_predictions):\n",
    "        # Predict the next point\n",
    "        next_point = loaded_model.predict(input_data)[:, -1, 0]\n",
    "        predictions.append(next_point)\n",
    "\n",
    "        # Append the predicted point to the input data\n",
    "        input_data = np.append(input_data[:, :, 1:], next_point.reshape(1, 1, 1), axis=2)\n",
    "\n",
    "    # Rescale the predicted values\n",
    "    rescaled_predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "\n",
    "    # Return the rescaled predicted sequence of points\n",
    "    return rescaled_predictions\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Assuming you have a trained model and a scaler object\n",
    "model = model\n",
    "scaler = min_max_scaler\n",
    "\n",
    "# Assuming you have the input data\n",
    "input_data = np.array([1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 3, 4, 5, 6, 7, 8, 9, 20, 21])\n",
    "\n",
    "# Saving the model and scaler\n",
    "folder_name = 'model_files'\n",
    "save_model_and_scaler(model, scaler)\n",
    "\n",
    "# Predicting the next points and rescaling\n",
    "predicted = predict_next_points(\n",
    "    os.path.join(folder_name, 'model.pkl'),\n",
    "    os.path.join(folder_name, 'scaler.pkl'),\n",
    "    input_data\n",
    ")\n",
    "\n",
    "# Printing the rescaled predictions\n",
    "print(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29.546246],\n",
       "       [29.380224],\n",
       "       [29.351662],\n",
       "       [29.412067],\n",
       "       [29.441767],\n",
       "       [29.547377],\n",
       "       [28.734745],\n",
       "       [28.565233],\n",
       "       [29.227211],\n",
       "       [29.209047],\n",
       "       [29.116936],\n",
       "       [28.954638],\n",
       "       [29.42141 ],\n",
       "       [28.636845],\n",
       "       [29.00268 ],\n",
       "       [28.994223],\n",
       "       [29.13886 ],\n",
       "       [29.127382],\n",
       "       [29.102297],\n",
       "       [28.583063],\n",
       "       [27.912226],\n",
       "       [27.911564],\n",
       "       [27.911356],\n",
       "       [27.911121]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load('model.pkl')\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the training and validation loss values from the history object\n",
    "# These lines extract the training loss and validation loss values from the history object, which was obtained during the model training.\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    " \n",
    "# Plot the training loss and validation loss\n",
    "# These lines create line plots for the training loss and validation loss values. The plot function is used to plot the values on a graph.\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "# These lines set the labels for the x-axis and y-axis, title of the plot, and enable the legend to display the labels of the plotted lines.\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Summary:\n",
    "# The code visualizes the training and validation loss values obtained during the model training. It plots the training loss and validation loss on the same graph, with the x-axis representing the epochs and the y-axis representing the loss values. The plot provides insights into the performance of the model over the training epochs, showing how the loss values change over time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines calculate the predictions (y_hat) using the trained model on the test data (x_test). \n",
    "# Then, the inverse transform is applied to y_hat and y_test using the min_max_scaler to obtain the actual\n",
    "# values in their original scale. The predicted values (y_hat_inversed) and actual \n",
    "# values (y_test_inversed) are converted to flat list\n",
    "Results_test = {}\n",
    "y_hat = model.predict(x_test)\n",
    "y_hat_inversed = min_max_scaler.inverse_transform(y_hat.reshape(-1,1)).flatten().tolist()\n",
    "y_test_inversed = min_max_scaler.inverse_transform(y_test.reshape(-1,1)).flatten().tolist()\n",
    "\n",
    "# These lines create a dictionary Results_test to store the predicted values, actual values, and index values.\n",
    "# The predicted and actual values are assigned to the corresponding keys, and the index values are generated using np.arange()\n",
    "Results_test['y_hat'] =y_hat_inversed\n",
    "Results_test['y_test'] =y_test_inversed\n",
    "Results_test['Index'] = np.arange(0, len(y_test_inversed))\n",
    "\n",
    "# These lines create a DataFrame results_df_test from the Results_test dictionary. The DataFrame is then displayed.\n",
    "results_df_test = pd.DataFrame.from_dict(Results_test)\n",
    "display(results_df_test)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=results_df_test['Index'], y=results_df_test['y_hat'], name='Predicted',mode='markers+lines'))\n",
    "fig.add_trace(go.Scatter(x=results_df_test['Index'], y=results_df_test['y_test'], name='Actual' ,mode='markers+lines'))\n",
    "fig.update_layout(\n",
    "            xaxis_title='Index',\n",
    "            yaxis_title='Value',\n",
    "            title='Tesing Data Predicted vs Actual Curves'\n",
    "        )\n",
    "fig.write_html('plots/LSTM-predicted_actual_curves.html')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Summary:\n",
    "# The code calculates the predictions of the LSTM model on the test data and transforms them back to their original scale.\n",
    "# It creates a DataFrame to store the predicted and actual values along with their corresponding indices. \n",
    "# Then, it generates a plot using Plotly with separate lines for the predicted and actual values.\n",
    "# The plot is saved as an HTML file and displayed. \n",
    "# This allows visual comparison between the predicted and actual values of the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Results_train = {}\n",
    "\n",
    "# for Training data \n",
    "y_train_hat = model.predict(x_train)\n",
    "y_train_hat_inversed = min_max_scaler.inverse_transform(y_train_hat.reshape(-1,1)).flatten().tolist()\n",
    "y_train_inversed = min_max_scaler.inverse_transform(y_train.reshape(-1,1)).flatten().tolist()\n",
    "\n",
    "Results_train['y_train_hat'] =y_train_hat_inversed\n",
    "Results_train['y_train'] =y_train_inversed\n",
    "Results_train['Index'] = np.arange(0, len(y_train_inversed))\n",
    "results_df_train = pd.DataFrame.from_dict(Results_train)\n",
    "display(results_df_train)\n",
    "\n",
    "\n",
    "\n",
    "# Plotting\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=results_df_train['Index'], y=results_df_train['y_train_hat'], name='Predicted',mode='markers+lines'))\n",
    "fig.add_trace(go.Scatter(x=results_df_train['Index'], y=results_df_train['y_train'], name='Actual' ,mode='markers+lines'))\n",
    "fig.update_layout(\n",
    "            xaxis_title='Index',\n",
    "            yaxis_title='Value',\n",
    "            title='Tesing Data Predicted vs Actual Curves'\n",
    "        )\n",
    "fig.write_html('plots/LSTM-predicted_actual_curves.html')\n",
    "fig.show()\n",
    "\n",
    "# Summary:\n",
    "# The code calculates the predictions of the LSTM model on the training data and transforms them back to their original scale. \n",
    "# It creates a DataFrame to store the predicted and actual values along with their corresponding indices \n",
    "# for the training data. Then, it generates a plot using Plotly with separate lines for the predicted and actual values. \n",
    "# The plot is saved as an HTML file and displayed. This allows visual comparison between the predicted and actual \n",
    "# values of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_errors(predicted, actual):\n",
    "    # Convert lists to numpy arrays\n",
    "    predicted = np.array(predicted)\n",
    "    actual = np.array(actual)\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    mae = np.mean(np.abs(predicted - actual))\n",
    "\n",
    "    # Root Mean Squared Error (RMSE)\n",
    "    rmse = np.sqrt(np.mean((predicted - actual) ** 2))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    mse = np.mean((predicted - actual) ** 2)\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "\n",
    "    return mae, rmse, mse, mape\n",
    "train_mae , train_rmse , train_mse,train_mape = calculate_errors(results_df_train['y_train_hat'],results_df_train['y_train'])\n",
    "test_mae , test_rmse , test_mse,test_mape = calculate_errors(results_df_test['y_hat'],results_df_test['y_test'])\n",
    "\n",
    "print('Results on Training Data')\n",
    "print(f'MAE : {train_mae}\\nRMSE : {train_rmse}\\nMSE : {train_mse}\\nMAPE : {train_mape}'.format())\n",
    "print('\\n')\n",
    "print('Results on Testing Data')\n",
    "print(f'MAE : {test_mae}\\nRMSE : {test_rmse}\\nMSE : {test_mse}\\nMAPE : {test_mape}')\n",
    "\n",
    "# Summary:\n",
    "# The code defines a function to calculate various error metrics (MAE, RMSE, MSE, MAPE) given predicted and actual values.\n",
    "# Then, the function is called to calculate the error metrics for both the training and testing data. \n",
    "# The calculated error metrics provide a quantitative assessment of the model's performance in terms of the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the existing error history DataFrame from CSV\n",
    "error_history = pd.read_csv('errorDatabaseHistory.csv')\n",
    "\n",
    "# Create a new record as a DataFrame\n",
    "new_record = pd.DataFrame({\n",
    "    'modelName': [model_name],\n",
    "    'modelConfig': [model.get_config()],\n",
    "    'train_rmse': [train_rmse],\n",
    "    'train_mae': [train_mae],\n",
    "    'train_mse': [train_mse],\n",
    "    'train_mape': [train_mape],\n",
    "    'test_rmse': [test_rmse],\n",
    "    'test_mae': [test_mae],\n",
    "    'test_mse': [test_mse],\n",
    "    'test_mape': [test_mape]\n",
    "})\n",
    "\n",
    "# Append the new record to the existing error history DataFrame\n",
    "error_history = pd.concat([error_history, new_record], ignore_index=True)\n",
    "\n",
    "# Save the updated error history DataFrame to CSV\n",
    "error_history.to_csv('errorDatabaseHistory.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_history.iloc[7][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
