{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # Library for data manipulation and analysis\n",
    "import numpy as np  # Library for numerical computing\n",
    "import glob  # Library for file handling\n",
    "import re  # Library for regular expressions\n",
    "from tqdm import tqdm  # Library for creating progress bars\n",
    "\n",
    "import plotly.graph_objects as go  # Library for creating interactive plots\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn import preprocessing  # Library for data preprocessing\n",
    "import keras  # Deep learning library\n",
    "from keras.models import Sequential  # Sequential model for stacking layers\n",
    "from keras.layers.core import Dense, Dropout, Activation  # Layers for fully connected neural networks\n",
    "from keras.layers import LSTM  # LSTM layer for sequence modeling\n",
    "from keras.models import load_model  # Loading pre-trained models\n",
    "import matplotlib.pyplot as plt  # Library for basic data visualization\n",
    "import h5py  # Library for handling large datasets\n",
    "import datetime  # Library for date and time operations\n",
    "import tensorflow as tf  # Deep learning library\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  # Import the TensorFlow library\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))  # Print the number of available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines use the glob module to find all file paths that match the pattern 'GreenD_reduced_version_03/*.csv'. \n",
    "# The resulting file paths are stored in path_lists.\n",
    "# Then, the file paths are sorted alphabetically and assigned to sorted_file_paths\n",
    "path_lists = glob.glob('GreenD_reduced_version_03/'+'*.csv')\n",
    "sorted_file_paths = sorted(path_lists)\n",
    "\n",
    "\n",
    "# This code defines a function extract_digits that takes a string as input and \n",
    "# uses regular expressions (re.findall()) to extract digits from the string. \n",
    "# It returns the extracted digits as an integer if they exist, otherwise it returns 0.\n",
    "# Then, the sorted() function is used with key=extract_digits to sort the file\n",
    "# paths based on the digits extracted from each path. Finally, the sorted file paths are printed\n",
    "def extract_digits(string):\n",
    "    # Extract digits from the string using regular expression\n",
    "    digits = re.findall(r'\\d+', string)\n",
    "    return int(digits[0]) if digits else 0\n",
    "\n",
    "sorted_file_paths = sorted(path_lists, key=extract_digits)\n",
    "print(sorted_file_paths)\n",
    "\n",
    "\n",
    "# In this block of code, an empty list conct_list is created. Then, a loop is executed over each sorted\n",
    "# file path using sorted_file_paths. Inside the loop, each CSV file is read using pd.read_csv(),\n",
    "# and the 'Timestamp' column is converted to a datetime object using pd.to_datetime() w\n",
    "# ith the utc=True parameter and unit='s' to indicate that the values are in Unix timestamp format. \n",
    "# The 'Timestamp' column is then formatted to a specific string format '%Y-%m-%d %H:%M:%S' \n",
    "# using the dt.strftime() function. The resulting dataframe is appended to the conct_list\n",
    "\n",
    "conct_list = []\n",
    "for path in tqdm(sorted_file_paths ,desc='processing'):\n",
    "    \n",
    "    data = pd.read_csv(path)\n",
    "    data['Timestamp'] = pd.to_datetime(data['Timestamp'],utc=True,unit='s')#format='%Y-%m-%d %H-%M-%S')\n",
    "    data['Timestamp'] = data['Timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    conct_list.append(data)\n",
    "    \n",
    "# The !mkdir command creates a directory named 'Combined-Dataset-version-03' \n",
    "# if it doesn't already exist. Then, pd.concat() is used to concatenate all \n",
    "# the dataframes in conct_list along the rows, creating a single combined dataframe named df\n",
    "!mkdir 'Combined-Dataset-version-03'\n",
    "df = pd.concat(conct_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='Timestamp',inplace=True)\n",
    "# This line of code sorts the DataFrame df based on the values in the 'Timestamp' column in ascending order. \n",
    "# The sort_values() function is used to perform the sorting operation. The by parameter specifies \n",
    "# the column to sort by, which in this case is 'Timestamp'. The inplace=True parameter is\n",
    "# used to modify the DataFrame df in place, meaning the changes are applied directly to the DataFrame instead of creating a new sorted DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('Concatenated_data',exist_ok=True)\n",
    "df.to_csv('Concatenated_data/Whole_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your DataFrame named 'df'\n",
    "\n",
    "# Downsample the DataFrame to 1-hour frequency and aggregate using sum\n",
    "hourly_df = perform_downsampling(df, freq='1H')\n",
    "\n",
    "# Downsample the DataFrame to daily frequency (24 hours) and aggregate using sum\n",
    "daily_df = perform_downsampling(df, freq='24H')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Assuming your time series data is stored in a DataFrame with a column named 'Timestamp' and a column named 'Value'\n",
    "# Set the 'Timestamp' column as the index of the DataFrame if it's not already set\n",
    "\n",
    "# Perform seasonal decomposition\n",
    "result = seasonal_decompose(hourly_df['Summe'], model='additive', period=24*150)\n",
    "\n",
    "# Access the trend, seasonal, and residual components\n",
    "trend = result.trend\n",
    "seasonal = result.seasonal\n",
    "residual = result.resid\n",
    "\n",
    "# Plot the original time series and the decomposed components\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(411)\n",
    "plt.plot(hourly_df['Summe'], label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal, label='Seasonal')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residual')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualization import plot\n",
    "plot(hourly_df,x_feature_name='Timestamp',y_feature_name='Summe',title='Sampling Frequency 1 Hours')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiDirectional LSTM Model Development Demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('Concatenated_data/Whole_data.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "data = data.set_index('Timestamp')\n",
    "\n",
    "# Create the input and output sequences\n",
    "input_seq = []\n",
    "output_seq = []\n",
    "\n",
    "# Convert data to numpy array\n",
    "values = data['Summe'].values\n",
    "\n",
    "# Number of previous days used as input\n",
    "num_input_days = 3\n",
    "\n",
    "# Number of future hours to forecast\n",
    "num_forecast_hours = 24\n",
    "\n",
    "# Create the input and output sequences\n",
    "for i in range(num_input_days, len(values) - num_forecast_hours):\n",
    "    input_seq.append(values[i - num_input_days:i])\n",
    "    output_seq.append(values[i:i + num_forecast_hours])\n",
    "\n",
    "# Convert the input and output sequences to numpy arrays\n",
    "input_seq = np.array(input_seq)\n",
    "output_seq = np.array(output_seq)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "input_seq = scaler.fit_transform(input_seq)\n",
    "output_seq = scaler.fit_transform(output_seq)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_input, test_input, train_output, test_output = train_test_split(input_seq, output_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Reshape the input sequences to 3D arrays\n",
    "train_input = train_input.reshape(-1, num_input_days, 1)\n",
    "test_input = test_input.reshape(-1, num_input_days, 1)\n",
    "\n",
    "# Define the parameters\n",
    "timesteps_per_day = num_input_days * 24\n",
    "past_input_timesteps = num_input_days\n",
    "batch_size = 64\n",
    "epochs = 75\n",
    "steps_per_epoch = 200\n",
    "lstm_units = 200\n",
    "dense_units = 130\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Build the bidirectional LSTM model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(lstm_units, return_sequences=True), input_shape=(past_input_timesteps, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(dense_units, activation='relu'))\n",
    "model.add(Dense(num_forecast_hours, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_input, train_output,verbose=2 ,epochs=epochs, batch_size=batch_size, steps_per_epoch=steps_per_epoch, validation_data=(test_input, test_output))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(test_input, test_output)\n",
    "print('Test Loss:', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_input[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_o = test_output[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(test)\n",
    "\n",
    "# Denormalize the predictions and actual values\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "actual_values = scaler.inverse_transform(test_o)\n",
    "\n",
    "# Plot the actual and predicted values\n",
    "plt.plot(actual_values.flatten(), label='Actual')\n",
    "plt.plot(predictions.flatten(), label='Predicted')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate sample input data\n",
    "num_samples = 1000\n",
    "input_data = np.random.rand(num_samples, past_input_timesteps * timesteps_per_day, 1)\n",
    "\n",
    "# # Generate sample output data\n",
    "# output_data = np.random.rand(num_samples, 24)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# train_samples = int(0.8 * num_samples)\n",
    "# train_input = input_data[:train_samples]\n",
    "# train_output = output_data[:train_samples]\n",
    "# test_input = input_data[train_samples:]\n",
    "# test_output = output_data[train_samples:]\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(train_input, train_output, batch_size=batch_size, epochs=epochs, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# predictions = model.predict(test_input)\n",
    "\n",
    "# # Plot the actual and predicted values\n",
    "# num_plots = 5  # Number of plots to display\n",
    "# plot_indices = np.random.choice(test_input.shape[0], num_plots, replace=False)  # Randomly select indices for plotting\n",
    "\n",
    "# fig, axes = plt.subplots(num_plots, figsize=(10, 6*num_plots))\n",
    "\n",
    "# for i, idx in enumerate(plot_indices):\n",
    "#     ax = axes[i]\n",
    "#     ax.plot(range(24), test_output[idx], label='Actual')\n",
    "#     ax.plot(range(24), predictions[idx], label='Predicted')\n",
    "#     ax.set_xlabel('Hour')\n",
    "#     ax.set_ylabel('Value')\n",
    "#     ax.set_title(f'Actual vs. Predicted - Sample {idx+1}')\n",
    "#     ax.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def create_dataset(dataset, look_back=30, predict_next=24):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back - predict_next):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        b = dataset[i+look_back:i+look_back+predict_next, 0]\n",
    "        dataY.append(b)\n",
    "        \n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sample = hourly_df.iloc[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# These lines perform feature scaling on the 'Summe' column of the DataFrame data_df \n",
    "# using Min-Max scaling. It creates an instance of MinMaxScaler and fits it to the data, \n",
    "# transforming the values to the range [0, 1]. The scaled data is assigned to the variable dataset\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = min_max_scaler.fit_transform(sub_sample['Summe'].values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split into train, validation, and test sets\n",
    "# These lines split the dataset into train, validation, and test sets based on specified proportions.\n",
    "# It calculates the sizes of each set and assigns the corresponding subsets of dataset to train_data, val_data, and test_data\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_data = dataset[:train_size]\n",
    "test_data = dataset[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create train, validation, and test datasets\n",
    "# These lines call the create_dataset function to create input-output pairs for the train, validation, \n",
    "# and test sets. The function takes the respective subsets of data (train_data, val_data, test_data) \n",
    "# and a look_back parameter, and returns input (x) and output (y) sequences. \n",
    "# The input-output pairs are assigned to x_train, y_train, x_val, y_val, x_test, and y_test\n",
    "look_back = 500\n",
    "x_train, y_train = create_dataset(train_data, look_back)\n",
    "# x_val, y_val = create_dataset(val_data, look_back)\n",
    "x_test, y_test = create_dataset(test_data, look_back)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reshape the input data\n",
    "# These lines reshape the input sequences (x_train, x_val, x_test) to match the required input shape for the LSTM model. \n",
    "# The new shape is (batch_size, timesteps, features), where batch_size is the number of samples,\n",
    "# timesteps is the number of time steps, and features is the number of features.\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "# x_val = np.reshape(x_val, (x_val.shape[0], 1, x_val.shape[1]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "\n",
    "# print the sizes of the datasets\n",
    "print('Training Data Size:', x_train.shape)\n",
    "# print('Validation Data Size:', x_val.shape)\n",
    "print('Testing Data Size:', x_test.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building | Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if GPU is available\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"GPU is available.\")\n",
    "    print(\"List of physical GPUs:\")\n",
    "    for device in physical_devices:\n",
    "        print(device)\n",
    "else:\n",
    "    print(\"GPU is not available. Using CPU instead.\")\n",
    "    \n",
    "    \n",
    "# Summary:\n",
    "# The code checks if a GPU is available and sets the memory growth option if it is.\n",
    "# It then prints the availability and a list of physical GPUs if present\n",
    "# . If no GPU is available, it prints a message indicating the use of the CPU instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Set the model parameters\n",
    "num_epochs = 200\n",
    "batch_size = 128\n",
    "num_layers = 4\n",
    "input_window =1000\n",
    "look_back = 1000\n",
    "\n",
    "# Create the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(input_window, input_shape=(1, look_back), return_sequences=True))\n",
    "model.add(LSTM(800,activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(500,activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(300,activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(200,activation='tanh', return_sequences=True))\n",
    "\n",
    "model.add(Dense(1))\n",
    "# Compile the model with a custom learning rate\n",
    "learning_rate = 0.001  # Change this value to your desired learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.MeanAbsoluteError(), optimizer=optimizer)\n",
    "\n",
    "# Get the model's parameters\n",
    "model_params = model.count_params()\n",
    "\n",
    "# Create the model name based on parameters and training settings\n",
    "model_name = f\"model_{num_epochs}_epochs_{batch_size}_batch_{num_layers}_layers_{model_params}_params_input_window_{input_window}\"\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(patience=10, monitor='loss', restore_best_weights=True)\n",
    "\n",
    "# Define TensorBoard callback with model-specific log folder\n",
    "tb_callback = TensorBoard(log_dir=f'logs/{model_name}/', write_graph=False, update_freq='epoch')\n",
    "\n",
    "# Define model checkpoint callback with model-specific filename\n",
    "checkpoint_callback = ModelCheckpoint(f'checkpoints/{model_name}_{{epoch:02d}}.h5', save_weights_only=True, save_best_only=True)\n",
    "\n",
    "# Train the model with early stopping, checkpoints, and TensorBoard\n",
    "history = model.fit(x_train, y_train, #validation_data=(x_val, y_val), \n",
    "                    epochs=num_epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stopping, \n",
    "                               tb_callback, \n",
    "                               checkpoint_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the training and validation loss values from the history object\n",
    "# These lines extract the training loss and validation loss values from the history object, which was obtained during the model training.\n",
    "train_loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    " \n",
    "# Plot the training loss and validation loss\n",
    "# These lines create line plots for the training loss and validation loss values. The plot function is used to plot the values on a graph.\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "# These lines set the labels for the x-axis and y-axis, title of the plot, and enable the legend to display the labels of the plotted lines.\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Summary:\n",
    "# The code visualizes the training and validation loss values obtained during the model training. It plots the training loss and validation loss on the same graph, with the x-axis representing the epochs and the y-axis representing the loss values. The plot provides insights into the performance of the model over the training epochs, showing how the loss values change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_input = x_train[-1].reshape(1, 1, 1000)\n",
    "predictions = []\n",
    "inverse_prediction_list = []\n",
    "\n",
    "for _ in range(100):\n",
    "    predicted = model.predict(current_input)\n",
    "    inverse_prediction = min_max_scaler.inverse_transform(predicted.reshape(-1, 1))\n",
    "    inverse_prediction_list.append(inverse_prediction.reshape(-1,1)[0][0])\n",
    "    predictions.append(predicted[0, -1, 0])\n",
    "    \n",
    "    # Update the input for the next step\n",
    "    current_input[0, :, :-1] = current_input[0, :, 1:]\n",
    "    current_input[0, :, -1] = predicted[0, -1, 0]\n",
    "    \n",
    "    # print(\"Prediction:\")\n",
    "    # print(predicted)\n",
    "    # print(\"Current input:\")\n",
    "    # print(current_input.reshape(-1, 1))\n",
    "    \n",
    "# Remove the first value from predictions\n",
    "# predictions = predictions[1:]\n",
    "inverse_prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(min_max_scaler.inverse_transform(test_data).reshape(-1,1).flatten().tolist()[0:100])\n",
    "plt.plot(inverse_prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_input = x_test[-1].reshape(1, 1, 1000)\n",
    "predictions = []\n",
    "inverse_prediction_list = []\n",
    "\n",
    "for _ in range(100):\n",
    "    predicted = model.predict(current_input)\n",
    "    inverse_prediction = min_max_scaler.inverse_transform(predicted.reshape(-1, 1))\n",
    "    inverse_prediction_list.append(inverse_prediction.reshape(-1,1)[0][0])\n",
    "    predictions.append(predicted[0, -1, 0])\n",
    "    \n",
    "    # Update the input for the next step\n",
    "    current_input[0, :, :-1] = current_input[0, :, 1:]\n",
    "    current_input[0, :, -1] = predicted[0, -1, 0]\n",
    "    \n",
    "    # print(\"Prediction:\")\n",
    "    # print(predicted)\n",
    "    # print(\"Current input:\")\n",
    "    # print(current_input.reshape(-1, 1))\n",
    "    \n",
    "# Remove the first value from predictions\n",
    "# predictions = predictions[1:]\n",
    "inverse_prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(min_max_scaler.inverse_transform(test_data).reshape(-1,1).flatten().tolist()[0:100])\n",
    "plt.plot(inverse_prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_input[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forecast_multi_step(model, test_data, lookback_window, num_steps, scaler):\n",
    "    forecasts = []\n",
    "    \n",
    "    for step in range(0, len(test_data) - num_steps + 1):\n",
    "        input_data = test_data[step:step + lookback_window]\n",
    "        scaled_input = scaler.transform(input_data)\n",
    "        \n",
    "        # Reshape the input data to match the model's input shape\n",
    "        reshaped_input = scaled_input.reshape(1, lookback_window, 1)\n",
    "        \n",
    "        # Make the prediction for each step\n",
    "        predictions = []\n",
    "        with_prediction = []\n",
    "        for i in range(num_steps):\n",
    "            prediction = model.predict(reshaped_input)\n",
    "            print(prediction)\n",
    "            with_prediction.append(scaler.inverse_transform(predictions.reshape(-1,1)))\n",
    "            predictions.append(prediction[0, -1, 0])\n",
    "            \n",
    "            # Update the input data for the next step\n",
    "            reshaped_input = np.roll(reshaped_input, -1, axis=1)\n",
    "            reshaped_input[0, -1, 0] = scaler.transform([[predictions[-1]]])\n",
    "        \n",
    "        # Inverse transform the predictions\n",
    "        predictions = scaler.inverse_transform(predictions)\n",
    "        \n",
    "        # Append the inverse transformed predictions to the forecasts list\n",
    "        forecasts.extend(predictions)\n",
    "    \n",
    "    return with_prediction\n",
    "\n",
    "\n",
    "lookback_window = 20\n",
    "num_steps = 24\n",
    "\n",
    "forecasts = forecast_multi_step(model, test_data, lookback_window, num_steps, min_max_scaler)\n",
    "\n",
    "print(\"Multi-step forecasts:\")\n",
    "print(forecasts)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler.inverse_transform(forecasts.reshape(-1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import joblib\n",
    "except:\n",
    "    !pip install joblib\n",
    "    import joblib\n",
    "    # Save the model\n",
    "joblib.dump(model, 'model.pkl')\n",
    "# Load the model\n",
    "loaded_model = joblib.load('model.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_model_and_scaler(model_path=None, scaler_path=None):\n",
    "    # Use default paths if not provided\n",
    "    model_path = model_path# or MODEL_PATH\n",
    "    scaler_path = scaler_path #or SCALER_PATH\n",
    "\n",
    "    # Load the model from the .pkl file\n",
    "    with open(model_path, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "\n",
    "    # Load the scaler from the .pkl file\n",
    "    with open(scaler_path, 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "\n",
    "    return loaded_model, scaler\n",
    "\n",
    "\n",
    "def predict_next_points(model_path, scaler_path, input_data, num_predictions=24):\n",
    "    \"\"\"\n",
    "    Predicts the next points in a sequence using the provided model.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the saved model file (.pkl).\n",
    "        scaler_path (str): Path to the saved scaler file (.pkl).\n",
    "        input_data (ndarray): Input sequence of points.\n",
    "        num_predictions (int): Number of points to predict. Default is 24.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Predicted sequence of points.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the model from the .pkl file\n",
    "    with open(model_path, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "\n",
    "    # Load the scaler from the .pkl file\n",
    "    with open(scaler_path, 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "\n",
    "    # Reshape the input data\n",
    "    input_data = np.array(input_data).reshape(1, 1, -1)\n",
    "\n",
    "    # Predict the next points iteratively\n",
    "    predictions = []\n",
    "    for _ in range(num_predictions):\n",
    "        # Predict the next point\n",
    "        next_point = loaded_model.predict(input_data)[:, -1, 0]\n",
    "        predictions.append(next_point)\n",
    "\n",
    "        # Append the predicted point to the input data\n",
    "        input_data = np.append(input_data[:, :, 1:], next_point.reshape(1, 1, 1), axis=2)\n",
    "\n",
    "    # Rescale the predicted values\n",
    "    rescaled_predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "\n",
    "    # Return the rescaled predicted sequence of points\n",
    "    return rescaled_predictions\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Assuming you have a trained model and a scaler object\n",
    "model = model\n",
    "scaler = min_max_scaler\n",
    "\n",
    "# Assuming you have the input data\n",
    "input_data = np.array([1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 3, 4, 5, 6, 7, 8, 9, 20, 21])\n",
    "\n",
    "# Saving the model and scaler\n",
    "folder_name = 'model_files'\n",
    "save_model_and_scaler(model, scaler)\n",
    "\n",
    "# Predicting the next points and rescaling\n",
    "predicted = predict_next_points(\n",
    "    os.path.join(folder_name, 'model.pkl'),\n",
    "    os.path.join(folder_name, 'scaler.pkl'),\n",
    "    input_data\n",
    ")\n",
    "\n",
    "# Printing the rescaled predictions\n",
    "print(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load('model.pkl')\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines calculate the predictions (y_hat) using the trained model on the test data (x_test). \n",
    "# Then, the inverse transform is applied to y_hat and y_test using the min_max_scaler to obtain the actual\n",
    "# values in their original scale. The predicted values (y_hat_inversed) and actual \n",
    "# values (y_test_inversed) are converted to flat list\n",
    "Results_test = {}\n",
    "y_hat = model.predict(x_test)\n",
    "y_hat_inversed = min_max_scaler.inverse_transform(y_hat.reshape(-1,1)).flatten().tolist()\n",
    "y_test_inversed = min_max_scaler.inverse_transform(y_test.reshape(-1,1)).flatten().tolist()\n",
    "\n",
    "# These lines create a dictionary Results_test to store the predicted values, actual values, and index values.\n",
    "# The predicted and actual values are assigned to the corresponding keys, and the index values are generated using np.arange()\n",
    "Results_test['y_hat'] =y_hat_inversed\n",
    "Results_test['y_test'] =y_test_inversed\n",
    "Results_test['Index'] = np.arange(0, len(y_test_inversed))\n",
    "\n",
    "# These lines create a DataFrame results_df_test from the Results_test dictionary. The DataFrame is then displayed.\n",
    "results_df_test = pd.DataFrame.from_dict(Results_test)\n",
    "display(results_df_test)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=results_df_test['Index'], y=results_df_test['y_hat'], name='Predicted',mode='markers+lines'))\n",
    "fig.add_trace(go.Scatter(x=results_df_test['Index'], y=results_df_test['y_test'], name='Actual' ,mode='markers+lines'))\n",
    "fig.update_layout(\n",
    "            xaxis_title='Index',\n",
    "            yaxis_title='Value',\n",
    "            title='Tesing Data Predicted vs Actual Curves'\n",
    "        )\n",
    "fig.write_html('plots/LSTM-predicted_actual_curves.html')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Summary:\n",
    "# The code calculates the predictions of the LSTM model on the test data and transforms them back to their original scale.\n",
    "# It creates a DataFrame to store the predicted and actual values along with their corresponding indices. \n",
    "# Then, it generates a plot using Plotly with separate lines for the predicted and actual values.\n",
    "# The plot is saved as an HTML file and displayed. \n",
    "# This allows visual comparison between the predicted and actual values of the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_test.plot(x='Index',y='y_hat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_test.plot(x='Index',y='y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Results_train = {}\n",
    "\n",
    "# for Training data \n",
    "y_train_hat = model.predict(x_train)\n",
    "y_train_hat_inversed = min_max_scaler.inverse_transform(y_train_hat.reshape(-1,1)).flatten().tolist()\n",
    "y_train_inversed = min_max_scaler.inverse_transform(y_train.reshape(-1,1)).flatten().tolist()\n",
    "\n",
    "Results_train['y_train_hat'] =y_train_hat_inversed\n",
    "Results_train['y_train'] =y_train_inversed\n",
    "Results_train['Index'] = np.arange(0, len(y_train_inversed))\n",
    "results_df_train = pd.DataFrame.from_dict(Results_train)\n",
    "display(results_df_train)\n",
    "\n",
    "\n",
    "\n",
    "# Plotting\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=results_df_train['Index'], y=results_df_train['y_train_hat'], name='Predicted',mode='markers+lines'))\n",
    "fig.add_trace(go.Scatter(x=results_df_train['Index'], y=results_df_train['y_train'], name='Actual' ,mode='markers+lines'))\n",
    "fig.update_layout(\n",
    "            xaxis_title='Index',\n",
    "            yaxis_title='Value',\n",
    "            title='Tesing Data Predicted vs Actual Curves'\n",
    "        )\n",
    "fig.write_html('plots/LSTM-predicted_actual_curves.html')\n",
    "fig.show()\n",
    "\n",
    "# Summary:\n",
    "# The code calculates the predictions of the LSTM model on the training data and transforms them back to their original scale. \n",
    "# It creates a DataFrame to store the predicted and actual values along with their corresponding indices \n",
    "# for the training data. Then, it generates a plot using Plotly with separate lines for the predicted and actual values. \n",
    "# The plot is saved as an HTML file and displayed. This allows visual comparison between the predicted and actual \n",
    "# values of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_errors(predicted, actual):\n",
    "    # Convert lists to numpy arrays\n",
    "    predicted = np.array(predicted)\n",
    "    actual = np.array(actual)\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    mae = np.mean(np.abs(predicted - actual))\n",
    "\n",
    "    # Root Mean Squared Error (RMSE)\n",
    "    rmse = np.sqrt(np.mean((predicted - actual) ** 2))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    mse = np.mean((predicted - actual) ** 2)\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "\n",
    "    return mae, rmse, mse, mape\n",
    "train_mae , train_rmse , train_mse,train_mape = calculate_errors(results_df_train['y_train_hat'],results_df_train['y_train'])\n",
    "test_mae , test_rmse , test_mse,test_mape = calculate_errors(results_df_test['y_hat'],results_df_test['y_test'])\n",
    "\n",
    "print('Results on Training Data')\n",
    "print(f'MAE : {train_mae}\\nRMSE : {train_rmse}\\nMSE : {train_mse}\\nMAPE : {train_mape}'.format())\n",
    "print('\\n')\n",
    "print('Results on Testing Data')\n",
    "print(f'MAE : {test_mae}\\nRMSE : {test_rmse}\\nMSE : {test_mse}\\nMAPE : {test_mape}')\n",
    "\n",
    "# Summary:\n",
    "# The code defines a function to calculate various error metrics (MAE, RMSE, MSE, MAPE) given predicted and actual values.\n",
    "# Then, the function is called to calculate the error metrics for both the training and testing data. \n",
    "# The calculated error metrics provide a quantitative assessment of the model's performance in terms of the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the existing error history DataFrame from CSV\n",
    "error_history = pd.read_csv('errorDatabaseHistory.csv')\n",
    "\n",
    "# Create a new record as a DataFrame\n",
    "new_record = pd.DataFrame({\n",
    "    'modelName': [model_name],\n",
    "    'modelConfig': [model.get_config()],\n",
    "    'train_rmse': [train_rmse],\n",
    "    'train_mae': [train_mae],\n",
    "    'train_mse': [train_mse],\n",
    "    'train_mape': [train_mape],\n",
    "    'test_rmse': [test_rmse],\n",
    "    'test_mae': [test_mae],\n",
    "    'test_mse': [test_mse],\n",
    "    'test_mape': [test_mape]\n",
    "})\n",
    "\n",
    "# Append the new record to the existing error history DataFrame\n",
    "error_history = pd.concat([error_history, new_record], ignore_index=True)\n",
    "\n",
    "# Save the updated error history DataFrame to CSV\n",
    "error_history.to_csv('errorDatabaseHistory.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Bi-directional LSTM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing: 100%|██████████| 472/472 [00:30<00:00, 15.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # Library for data manipulation and analysis\n",
    "import numpy as np  # Library for numerical computing\n",
    "import glob  # Library for file handling\n",
    "import re  # Library for regular expressions\n",
    "from tqdm import tqdm  # Library for creating progress bars\n",
    "\n",
    "import plotly.graph_objects as go  # Library for creating interactive plots\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn import preprocessing  # Library for data preprocessing\n",
    "import keras  # Deep learning library\n",
    "from keras.models import Sequential  # Sequential model for stacking layers\n",
    "from keras.layers.core import Dense, Dropout, Activation  # Layers for fully connected neural networks\n",
    "from keras.layers import LSTM  # LSTM layer for sequence modeling\n",
    "from keras.models import load_model  # Loading pre-trained models\n",
    "import matplotlib.pyplot as plt  # Library for basic data visualization\n",
    "import h5py  # Library for handling large datasets\n",
    "import datetime  # Library for date and time operations\n",
    "import tensorflow as tf  # Deep learning library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "from joblib import dump, load\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "\n",
    "# Creating Folder For sasving model files\n",
    "\n",
    "os.makedirs('ModelFiles',exist_ok=True)\n",
    "\n",
    "def extract_digits(string):\n",
    "    # Extract digits from the string using regular expression\n",
    "    digits = re.findall(r'\\d+', string)\n",
    "    return int(digits[0]) if digits else 0\n",
    "\n",
    "\n",
    "def perform_downsampling(data, freq, aggregation_func='sum'):\n",
    "    # Create a copy of the original data to avoid modifying it directly\n",
    "    df = data.copy()\n",
    "\n",
    "    # Check if 'Timestamp' column is already a datetime index\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        # Convert 'Timestamp' column to datetime index\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "    # Downsample the DataFrame to the specified frequency and apply the aggregation function\n",
    "    downsampled_df = df.resample(freq).agg(aggregation_func)\n",
    "\n",
    "    # Fill missing values using forward fill\n",
    "    downsampled_df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # Reset the index of the downsampled DataFrame\n",
    "    downsampled_df.reset_index(inplace=True)\n",
    "\n",
    "    return downsampled_df\n",
    "\n",
    "\n",
    "def plot(df,x_feature_name,y_feature_name,title):\n",
    "    \"\"\"\n",
    "    This function takes two dataframes as input and plots the number of calls per day and per week.\n",
    "\n",
    "    Args:\n",
    "    daily_df (pandas.DataFrame): A dataframe containing daily call data.\n",
    "    weekly_df (pandas.DataFrame): A dataframe containing weekly call data.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # A new instance of the go.Figure() class from the plotly.graph_objects library is created. This will be used to create the plot\n",
    "    fig = go.Figure()\n",
    "    # Add a trace for daily calls\n",
    "    # A trace is added to the figure using the go.Scatter() class from plotly.graph_objects. \n",
    "    # It specifies the x and y data for the plot, assigns a name to the trace, \n",
    "    # and sets the mode to display lines and markers.\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[x_feature_name],\n",
    "            y=df[y_feature_name],\n",
    "            name=y_feature_name,\n",
    "            mode='lines+markers'\n",
    "        ))\n",
    "\n",
    " \n",
    "\n",
    "    # Update xaxis properties\n",
    "    # The x-axis and y-axis titles are updated using the update_xaxes() and update_yaxes() methods of the figure object.\n",
    "    fig.update_xaxes(title_text='Date')\n",
    "\n",
    "    # Update yaxis properties\n",
    "    fig.update_yaxes(title_text=y_feature_name)\n",
    "\n",
    "    # Update title and height\n",
    "    # The layout of the figure is updated using the update_layout() method. The title, height, and width of the plot are set.\n",
    "    fig.update_layout(\n",
    "        title=f'{title}',\n",
    "        height=500,\n",
    "        width=1200\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    # The plot is displayed using the show() method of the figure object.\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def prepare_data(num_input_days, num_forecast_hours,data ):\n",
    "    # Load the data\n",
    "    data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "    data = data.set_index('Timestamp')\n",
    "\n",
    "    # Create the input and output sequences\n",
    "    input_seq = []\n",
    "    output_seq = []\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Convert data to numpy array\n",
    "    values = data['Summe'].values\n",
    "\n",
    "    values = scaler.fit_transform(values.reshape(-1,1))\n",
    "      # Save it to a file\n",
    "    dump(scaler, 'ModelFiles/scaler.pkl')\n",
    "    \n",
    "    # Create the input and output sequences\n",
    "    for i in range(num_input_days, len(values) - num_forecast_hours):\n",
    "        input_seq.append(values[i - num_input_days:i])\n",
    "        output_seq.append(values[i:i + num_forecast_hours])\n",
    "\n",
    "    # Convert the input and output sequences to numpy arrays\n",
    "    input_seq = np.array(input_seq)\n",
    "    output_seq = np.array(output_seq)\n",
    "\n",
    "   \n",
    "    \n",
    "  \n",
    "    # input_seq = scaler.fit_transform(input_seq)\n",
    "    # output_seq = scaler.fit_transform(output_seq)\n",
    "\n",
    "    # Split the data into training, validation and testing sets\n",
    "    input_seq_train_val, input_seq_test, output_seq_train_val, output_seq_test = train_test_split(input_seq, output_seq, test_size=0.2, shuffle=False)\n",
    "    train_input, val_input, train_output, val_output = train_test_split(input_seq_train_val, output_seq_train_val, test_size=0.25, shuffle=False)\n",
    "\n",
    "    # Reshape the input sequences to 3D arrays\n",
    "    train_input = train_input.reshape(-1, num_input_days, 1)\n",
    "    val_input = val_input.reshape(-1, num_input_days, 1)\n",
    "    test_input = input_seq_test.reshape(-1, num_input_days, 1)\n",
    "\n",
    "    return train_input, val_input, test_input, train_output, val_output, output_seq_test\n",
    "\n",
    "def create_model(num_input_days, lstm_units, dense_units, learning_rate,input_shape):\n",
    "    # Build the bidirectional LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(lstm_units, return_sequences=True), input_shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Dense(num_forecast_hours, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "def train_and_evaluate_model(model, train_input, train_output, val_input, val_output, batch_size, epochs, steps_per_epoch):\n",
    "    # Train the model\n",
    "\n",
    "    filepath = 'ModelFiles/models/{epoch:02d}-{loss:.4f}-{val_loss:.4f}-{mae:.4f}-{val_mae:.4f}.pkl'\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=10),\n",
    "                ModelCheckpoint(filepath, monitor='loss', save_best_only=True, mode='min')]\n",
    "    \n",
    "\n",
    "    history = model.fit(train_input, train_output,\n",
    "                        epochs=epochs, batch_size=batch_size,\n",
    "                        steps_per_epoch=steps_per_epoch, \n",
    "                        validation_data=(val_input, val_output),\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "    dump(model, 'ModelFiles/bidirectionalLstm.pkl')\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss = model.evaluate(val_input, val_output)\n",
    "    print('Validation Loss:', loss)\n",
    "\n",
    "    return history, loss\n",
    "def plot_loss(history):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "def create_features(df, timestamp_col):\n",
    "    \"\"\"\n",
    "    Create time series features from datetime index.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataframe\n",
    "    timestamp_col (str): Name of timestamp column\n",
    "    \n",
    "    Returns:\n",
    "    df (pandas.DataFrame): Dataframe with added features.\n",
    "    \"\"\"\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "    \n",
    "    # Create new time related features\n",
    "    df['Year'] = df[timestamp_col].dt.year\n",
    "    df['Month'] = df[timestamp_col].dt.month\n",
    "    df['Day'] = df[timestamp_col].dt.day\n",
    "    df['Hour'] = df[timestamp_col].dt.hour\n",
    "    df['Minute'] = df[timestamp_col].dt.minute\n",
    "    df['DayOfWeek'] = df[timestamp_col].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    df['Weekend'] = df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)  # 1 if the day is weekend\n",
    "\n",
    "    # Consider holidays if your data has specific trends on holidays\n",
    "    cal = calendar()\n",
    "    holidays = cal.holidays(start=df[timestamp_col].min(), end=df[timestamp_col].max())\n",
    "    df['Holiday'] = df[timestamp_col].isin(holidays).astype(int)  # 1 if the day is a US Federal holiday\n",
    "\n",
    "    # You may also consider to create a feature that represents the time of the day (morning, afternoon, evening, night)\n",
    "    df['TimeOfDay'] = pd.cut(df['Hour'], bins=[-0.1, 6, 12, 18, 24], labels=['Night', 'Morning', 'Afternoon', 'Evening'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def evaluate_model(model, test_input, test_output, scaler):\n",
    "    # Predict the output for the test input\n",
    "    test_predictions = model.predict(test_input)\n",
    "\n",
    "    print(f\"test_predictions.shape: {test_predictions.shape}\")\n",
    "    print(f\"test_output.shape: {test_output.shape}\")\n",
    "\n",
    "    # Reshape test predictions and test output to be suitable for inverse transform\n",
    "    test_predictions_2D = test_predictions.reshape((test_predictions.shape[0] * test_predictions.shape[1], 1))\n",
    "    test_output_2D = test_output.reshape((test_output.shape[0] * test_output.shape[1], 1))\n",
    "    \n",
    "    print(f\"test_predictions_2D.shape: {test_predictions_2D.shape}\")\n",
    "    print(f\"test_output_2D.shape: {test_output_2D.shape}\")\n",
    "\n",
    "    # Inverse transform the predictions\n",
    "    test_predictions = scaler.inverse_transform(test_predictions_2D)\n",
    "    test_predictions = test_predictions.reshape(test_output.shape)\n",
    "\n",
    "    # Inverse transform the actual test output\n",
    "    test_output_actual = scaler.inverse_transform(test_output_2D)\n",
    "    test_output_actual = test_output_actual.reshape(test_output.shape)\n",
    "\n",
    "    print(f\"test_predictions.shape after reshaping: {test_predictions.shape}\")\n",
    "    print(f\"test_output_actual.shape after reshaping: {test_output_actual.shape}\")\n",
    "\n",
    "    # Calculate Mean Absolute Error\n",
    "    mae = mean_absolute_error(test_output_actual, test_predictions)\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "    # Calculate Mean Absolute Percentage Error\n",
    "    mape = mean_absolute_percentage_error(test_output_actual, test_predictions)\n",
    "    print(f\"Mean Absolute Percentage Error: {mape}\")\n",
    "\n",
    "def create_Features_Scaling(hourly_df,num_input_days=72,num_forecast_hours=1):\n",
    "    Year = []\n",
    "    Month = []\n",
    "    Day = []\n",
    "    Hour = []\n",
    "    Minute = []\n",
    "    DayOfWeek = []\n",
    "    Weekend = []\n",
    "    Holiday = []\n",
    "    TimeOfDay = []\n",
    "    Summe = []\n",
    "    Summe_ahead = []\n",
    "    for index  in range(0,hourly_df.shape[0]-num_input_days-num_forecast_hours):\n",
    "        \n",
    "        # try:\n",
    "            \n",
    "            Year.append(hourly_df.loc[index:index+num_input_days,'Year'])\n",
    "            Month.append(hourly_df.loc[index:index+num_input_days,'Month'])\n",
    "            Day.append(hourly_df.loc[index:index+num_input_days,'Day'])\n",
    "            Hour.append(hourly_df.loc[index:index+num_input_days,'Hour'])\n",
    "            Minute.append(hourly_df.loc[index:index+num_input_days,'Minute'])\n",
    "            DayOfWeek.append(hourly_df.loc[index:index+num_input_days,'DayOfWeek'])\n",
    "            Weekend.append(hourly_df.loc[index:index+num_input_days,'Weekend'])\n",
    "            Holiday.append(hourly_df.loc[index:index+num_input_days,'Holiday'])\n",
    "            TimeOfDay.append(hourly_df.loc[index:index+num_input_days,'TimeOfDay'])\n",
    "            Summe.append(hourly_df.loc[index:index+num_input_days,'Summe'])\n",
    "            Summe_ahead.append(hourly_df.loc[index+num_input_days+num_forecast_hours,'Summe'])\n",
    "        # except:\n",
    "        #     continue\n",
    "\n",
    "    Summe =  np.array(Summe)\n",
    "    Year =  np.array(Year)\n",
    "    Month =  np.array(Month)\n",
    "    Day =  np.array(Day)\n",
    "    Hour =  np.array(Hour)\n",
    "    Minute =  np.array(Minute)\n",
    "    DayOfWeek =  np.array(DayOfWeek)\n",
    "    Weekend =  np.array(Weekend)\n",
    "    Holiday =  np.array(Holiday)\n",
    "    TimeOfDay =  np.array(TimeOfDay)\n",
    "\n",
    "    # Target variables \n",
    "    Summe_ahead =  np.array(Summe_ahead)\n",
    "    Summe_ahead = np.reshape(Summe_ahead,(len(Summe_ahead),num_forecast_hours))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    minMaxScaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "    Summe_scaler = minMaxScaler.fit_transform(Summe)\n",
    "    Year_scaler = minMaxScaler.fit_transform(Year)\n",
    "    Month_scaler = minMaxScaler.fit_transform(Month)\n",
    "    Day_scaler = minMaxScaler.fit_transform(Day)\n",
    "    Minute_scaler = minMaxScaler.fit_transform(Minute)\n",
    "    DayOfWeek_scaler = minMaxScaler.fit_transform(DayOfWeek)\n",
    "    Weekend_scaler = minMaxScaler.fit_transform(Weekend)\n",
    "    Holiday_scaler = minMaxScaler.fit_transform(Holiday)\n",
    "    TimeOfDay_scaler = minMaxScaler.fit_transform(TimeOfDay)\n",
    "    Summe_ahead_scaler = minMaxScaler.fit_transform(Summe_ahead)\n",
    "    \n",
    "    dump(minMaxScaler, 'ModelFiles/scaler.pkl')\n",
    "\n",
    "    X =  np.stack([Summe_scaler, Year_scaler,Month_scaler,Day_scaler,Minute_scaler,DayOfWeek_scaler,Weekend_scaler,Holiday_scaler,TimeOfDay_scaler],axis=2)\n",
    "    Y =  Summe_ahead_scaler\n",
    "    \n",
    "    input_seq_train_val, input_seq_test, output_seq_train_val, output_seq_test = train_test_split(X, Y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    \n",
    "    return input_seq_train_val, input_seq_test, output_seq_train_val, output_seq_test\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    \n",
    "    # # loading the datasets from different files \n",
    "    path_lists = glob.glob('GreenD_reduced_version_03/'+'*.csv')\n",
    "    sorted_file_paths = sorted(path_lists)\n",
    "    sorted_file_paths = sorted(path_lists, key=extract_digits)\n",
    "    \n",
    "    conct_list = []\n",
    "    for path in tqdm(sorted_file_paths ,desc='processing'):\n",
    "        \n",
    "        data = pd.read_csv(path)\n",
    "        data['Timestamp'] = pd.to_datetime(data['Timestamp'],utc=True,unit='s')#format='%Y-%m-%d %H-%M-%S')\n",
    "        data['Timestamp'] = data['Timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        conct_list.append(data)\n",
    "        \n",
    "    comnbineDf = pd.concat(conct_list)\n",
    "    comnbineDf.sort_values(by='Timestamp',inplace=True)\n",
    "\n",
    "    # os.makedirs('HourlyData',exist_ok=True)\n",
    "  \n",
    "    hourly_df = perform_downsampling(comnbineDf, freq='1H')\n",
    "    # hourly_df.to_csv('HourlyData/hourlyDf.csv',index=False)\n",
    "    \n",
    "    # hourly_df =  pd.read_csv('HourlyData/hourlyDf.csv')\n",
    "    hourly_df =  create_features(hourly_df,'Timestamp')\n",
    "    hourly_df['TimeOfDay'] = hourly_df['TimeOfDay'].map({'Night': 0, 'Morning': 1, 'Afternoon': 2, 'Evening': 3})\n",
    "    \n",
    "    num_input_days = 24*3\n",
    "    num_forecast_hours = 1\n",
    "    timesteps_per_day = num_input_days * 24\n",
    "    past_input_timesteps = num_input_days\n",
    "    \n",
    "     \n",
    "    input_seq_train_val, input_seq_test, output_seq_train_val, output_seq_test= create_Features_Scaling(hourly_df,num_input_days,num_forecast_hours)\n",
    "    train_input, val_input, train_output, val_output = train_test_split(input_seq_train_val, output_seq_train_val, test_size=0.25, shuffle=False)\n",
    "\n",
    "    \n",
    "    batch_size = 64\n",
    "    epochs = 75\n",
    "\n",
    "    steps_per_epoch = 200\n",
    "    lstm_units = 200\n",
    "    dense_units = 130\n",
    "    learning_rate = 0.001\n",
    "    input_shape = (train_input.shape[1] , train_input.shape[2])\n",
    "    # model = create_model(past_input_timesteps, lstm_units, dense_units, learning_rate,input_shape)\n",
    "        \n",
    "    # history, loss = train_and_evaluate_model(model, train_input, train_output, val_input, val_output, batch_size, epochs, steps_per_epoch)\n",
    "    \n",
    "    # plot_loss(history)\n",
    "# # \n",
    "\n",
    "\n",
    "#     # Load the saved scaler\n",
    "    # scaler = load('ModelFiles/scaler.pkl')\n",
    "    # model = load('ModelFiles/bidirectionalLstm.pkl')\n",
    "\n",
    "    # evaluate_model(model, input_seq_test, output_seq_test, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " num_input_days = 24*3\n",
    "num_forecast_hours = 1\n",
    "timesteps_per_day = num_input_days * 24\n",
    "past_input_timesteps = num_input_days\n",
    "\n",
    "\n",
    "input_seq_train_val, input_seq_test, output_seq_train_val, output_seq_test= create_Features_Scaling(hourly_df,num_input_days,num_forecast_hours)\n",
    "train_input, val_input, train_output, val_output = train_test_split(input_seq_train_val, output_seq_train_val, test_size=0.25, shuffle=False)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 75\n",
    "\n",
    "steps_per_epoch = 200\n",
    "lstm_units = 200\n",
    "dense_units = 130\n",
    "learning_rate = 0.001\n",
    "input_shape = (train_input.shape[1] , train_input.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_5 (Bidirectio  (None, 73, 400)          336000    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 29200)             0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 130)               3796130   \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 131       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,132,261\n",
      "Trainable params: 4,132,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 19:27:02.443508: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-08 19:27:02.444517: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-08 19:27:02.445804: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-08 19:27:02.555639: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-08 19:27:02.592222: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-08 19:27:02.593106: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-08 19:27:02.594162: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(lstm_units, return_sequences=True), input_shape=input_shape))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(dense_units, activation='relu'))\n",
    "model.add(Dense(num_forecast_hours, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 19:39:58.646965: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-08 19:39:58.649023: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-08 19:39:58.650781: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-08 19:39:58.819104: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-08 19:39:58.895200: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-08 19:39:58.897182: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-08 19:39:58.898820: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-08 19:39:59.433139: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-08 19:39:59.940658: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-08 19:39:59.942551: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-08 19:39:59.944123: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-08 19:40:00.146447: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-08 19:40:00.224038: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-08 19:40:00.225779: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-08 19:40:00.227382: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-08 19:40:00.875121: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - ETA: 0s - loss: 0.0309"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 19:40:30.912222: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-08 19:40:30.916404: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-08 19:40:30.920988: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-08 19:40:31.189619: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-08 19:40:31.289089: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-08 19:40:31.291324: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-08 19:40:31.293570: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 36s 306ms/step - loss: 0.0309 - val_loss: 0.0396\n",
      "Epoch 2/75\n",
      "107/107 [==============================] - 34s 322ms/step - loss: 0.0306 - val_loss: 0.0317\n",
      "Epoch 3/75\n",
      "107/107 [==============================] - 34s 320ms/step - loss: 0.0304 - val_loss: 0.0339\n",
      "Epoch 4/75\n",
      "107/107 [==============================] - 33s 313ms/step - loss: 0.0305 - val_loss: 0.0313\n",
      "Epoch 5/75\n",
      "107/107 [==============================] - 37s 346ms/step - loss: 0.0301 - val_loss: 0.0313\n",
      "Epoch 6/75\n",
      "107/107 [==============================] - 31s 289ms/step - loss: 0.0295 - val_loss: 0.0333\n",
      "Epoch 7/75\n",
      "107/107 [==============================] - 28s 258ms/step - loss: 0.0302 - val_loss: 0.0706\n",
      "Epoch 8/75\n",
      "107/107 [==============================] - 26s 247ms/step - loss: 0.0316 - val_loss: 0.0323\n",
      "Epoch 9/75\n",
      "107/107 [==============================] - 26s 239ms/step - loss: 0.0314 - val_loss: 0.0324\n",
      "Epoch 10/75\n",
      "107/107 [==============================] - 26s 248ms/step - loss: 0.0315 - val_loss: 0.0325\n",
      "Epoch 11/75\n",
      "107/107 [==============================] - 25s 238ms/step - loss: 0.0315 - val_loss: 0.0323\n",
      "Epoch 12/75\n",
      "107/107 [==============================] - 26s 240ms/step - loss: 0.0315 - val_loss: 0.0324\n",
      "Epoch 13/75\n",
      "107/107 [==============================] - 26s 247ms/step - loss: 0.0315 - val_loss: 0.0326\n",
      "Epoch 14/75\n",
      "107/107 [==============================] - 26s 245ms/step - loss: 0.0314 - val_loss: 0.0324\n",
      "Epoch 15/75\n",
      "107/107 [==============================] - 26s 244ms/step - loss: 0.0315 - val_loss: 0.0323\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.MeanAbsoluteError(), optimizer=optimizer)\n",
    "\n",
    "# Get the model's parameters\n",
    "model_params = model.count_params()\n",
    "\n",
    "# Create the model name based on parameters and training settings\n",
    "model_name = f\"model_{epochs}_epochs_{batch_size}_{model_params}_params_input_window_{num_input_days}\"\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
    "\n",
    "# Define TensorBoard callback with model-specific log folder\n",
    "tb_callback = TensorBoard(log_dir=f'logs/{model_name}/', write_graph=False, update_freq='epoch')\n",
    "\n",
    "# Define model checkpoint callback with model-specific filename\n",
    "checkpoint_callback = ModelCheckpoint(f'checkpoints/{model_name}_{{epoch:02d}}.h5', save_weights_only=True, save_best_only=True)\n",
    "\n",
    "# Train the model with early stopping, checkpoints, and TensorBoard\n",
    "history = model.fit(train_input, train_output, #validation_data=(x_val, y_val), \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    verbose=1,\n",
    "                    validation_data=(val_input, val_output),\n",
    "                    callbacks=[early_stopping, \n",
    "                               tb_callback, \n",
    "                               checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df.to_csv('HourlyData/hourlyDf.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional\n",
    "\n",
    "# Generate fake data\n",
    "np.random.seed(0)\n",
    "n_samples = 1000\n",
    "n_timesteps = 24\n",
    "n_features = 5\n",
    "X = np.random.rand(n_samples, n_timesteps, n_features)\n",
    "y = np.random.rand(n_samples, 1)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_size = int(0.7 * n_samples)\n",
    "val_size = int(0.2 * n_samples)\n",
    "test_size = n_samples - train_size - val_size\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# Define and train the LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(n_timesteps, n_features)))\n",
    "lstm_model.add(Bidirectional(LSTM(32)))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(loss='mse', optimizer='adam')\n",
    "lstm_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "# Generate LSTM predictions\n",
    "lstm_preds = lstm_model.predict(X_test)\n",
    "\n",
    "# Define and train the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "xgb_model.fit(lstm_preds, y_test)\n",
    "\n",
    "# Generate XGBoost predictions\n",
    "xgb_preds = xgb_model.predict(lstm_preds)\n",
    "\n",
    "# Combine LSTM and XGBoost predictions\n",
    "final_preds = lstm_preds + xgb_preds.reshape(-1, 1)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = np.mean((final_preds - y_test)**2)\n",
    "print('Mean Squared Error:', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_train, label='Training', color='green')\n",
    "plt.plot(np.concatenate([y_train[-1:], y_val]), label='Validation', color='orange')\n",
    "plt.plot(y_test, label='Test', color='blue')\n",
    "\n",
    "# Plot the actual and predicted load values\n",
    "plt.plot(y_test, label='Actual', color='black')\n",
    "plt.plot(final_preds, label='Predicted', color='red')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Load (kW)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyBrid BiDirectional LSTM Model With XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "# Assume that 'df' is your DataFrame\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('HourlyData/hourlyDf.csv')  # replace 'your_file.csv' with your file name\n",
    "df['TimeOfDay'] = df['TimeOfDay'].map({'Night': 0, 'Morning': 1, 'Afternoon': 2, 'Evening': 3})\n",
    "\n",
    "\n",
    "data = df.drop(['Timestamp', 'Summe'], axis=1)\n",
    "X = data\n",
    "y = df['Summe']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "scaler_X.fit(X_train)\n",
    "scaler_y.fit(y_train.values.reshape(-1,1))\n",
    "\n",
    "X_train = scaler_X.transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "X_val = scaler_X.transform(X_val)\n",
    "\n",
    "y_train = scaler_y.transform(y_train.values.reshape(-1,1))\n",
    "y_test = scaler_y.transform(y_test.values.reshape(-1,1))\n",
    "y_val = scaler_y.transform(y_val.values.reshape(-1,1))\n",
    "\n",
    "n_future = 24\n",
    "n_past = 24*3\n",
    "\n",
    "X_train_seq = []\n",
    "y_train_seq = []\n",
    "\n",
    "# Modify the sequence formation process\n",
    "for i in range(n_past, len(X_train) - n_future + 1):\n",
    "    X_train_seq.append(X_train[i - n_past:i])\n",
    "    y_train_seq.append(y_train[i + n_future - 1])\n",
    "\n",
    "X_train, y_train = np.array(X_train_seq), np.array(y_train_seq)\n",
    "X_train, y_train = np.array(X_train_seq), np.array(y_train_seq)\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "lstm_model.add(Bidirectional(LSTM(32)))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "history = lstm_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=2, batch_size=32)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "lstm_preds_train = lstm_model.predict(X_train)\n",
    "lstm_preds_test = lstm_model.predict(X_test)\n",
    "\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "xgb_model.fit(X_train.reshape(X_train.shape[0], -1), y_train)  # Train on original features\n",
    "\n",
    "xgb_preds = xgb_model.predict(X_test.reshape(X_test.shape[0], -1))  # Predict on original features\n",
    "\n",
    "final_preds = lstm_preds_test + xgb_preds.reshape(-1, 1)\n",
    "\n",
    "final_preds_inv = scaler_y.inverse_transform(final_preds)  # Inverse transform to get original scale\n",
    "y_test_inv = scaler_y.inverse_transform(y_test)  # Inverse transform to get original scale\n",
    "\n",
    "mse = mean_squared_error(y_test_inv[:len(final_preds)], final_preds_inv)  # Make sure to align length of y_test\n",
    "print('Mean Squared Error:', mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
